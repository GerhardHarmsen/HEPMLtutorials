{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on : introduction to BDT on HEP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "QdmaHjz-xCm-",
    "outputId": "baaa6b72-f948-4780-ffb3-f67db79eb72f"
   },
   "outputs": [],
   "source": [
    "#COLAB=False #if running on local anaconda installation https://docs.anaconda.com/anaconda/install/\n",
    "COLAB=True #if running on https://colab.research.google.com/notebooks/welcome.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()\n",
    "np.random.seed(31415) # set the random seed for the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "QdmaHjz-xCm-",
    "outputId": "baaa6b72-f948-4780-ffb3-f67db79eb72f"
   },
   "outputs": [],
   "source": [
    "# Fix environment if necessary\n",
    "if COLAB:\n",
    "    !pip install eli5 # permutation importance package, now obsolete but good enough\n",
    "    pass\n",
    "else:    \n",
    "    # install xgboost and lighgbm, two popular Boosted Decision Tree packages\n",
    "    # normally it is just a matter of a few pip install but there might be some glitches\n",
    "    # note : I have better experience with pip install than conda install. \n",
    "    # the following need to be done only once. To be commented out later\n",
    "    #!pip install xgboost # might need to run !pip install cmake first   \n",
    "    #!pip install lightgbm # might need to run before brew install libopm on a mac\n",
    "    #!pip install eli5\n",
    "    pass\n",
    "\n",
    "import xgboost\n",
    "print (xgboost.__version__) # Tested with 1.0.2, version above 1 is recommended. Colab has 0.9.0, good enough \n",
    "import lightgbm\n",
    "print (lightgbm.__version__) # Tested with 2.3.1\n",
    "import eli5\n",
    "print (eli5.__version__) # Tested with 0.10.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "IsChk6u-r7fk",
    "outputId": "68642c78-72f7-49e3-92fb-c659c4f71300",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    #### Reading files from Google Drive\n",
    "    # one need a google account to be identified\n",
    "    # select a google account, then cut and paste the long password in the pop up field\n",
    "    !pip install PyDrive\n",
    "    import os\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "kl-4W4Ifs8EV",
    "outputId": "e9d9ca16-dc43-4c6b-f994-fcccf5d62be7"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "\n",
    "    #attach dataset from google drive \n",
    "    download = drive.CreateFile({'id': '1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4'})\n",
    "    download.GetContentFile(\"dataWW_d1_600k.csv.gz\")\n",
    "\n",
    "\n",
    "    datapath=\"\"\n",
    "\n",
    "\n",
    "    !ls -lrt\n",
    "else :\n",
    "    # make sure the file is available locally. \n",
    "    #Should be downloaded from https://drive.google.com/open?id=1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4\n",
    "    !ls -lrt # what is in the local directory\n",
    "    datapath=\"/Users/rousseau/Google\\ Drive/GD_openData/dataWW_ATLAS_openData13TeV_filtered/\"\n",
    "\n",
    "    !ls -lrt {datapath} # what is in the data directory\n",
    "    datapath=os.path.abspath(datapath).replace(\"\\ \", \" \")  # try to normalise the path (annoyance with the space)\n",
    "    print (\"Will take data from : \",datapath)\n",
    "\n",
    "filename=os.path.join(datapath,\"dataWW_d1_600k.csv.gz\")\n",
    "#load data\n",
    "# data was created from ATLAS Open Data see doc\n",
    "# http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html\n",
    "dfall = pd.read_csv(filename) \n",
    "\n",
    "#shuffle the events, already done but just to be safe\n",
    "dfall = dfall.sample(frac=1).reset_index(drop=True)\n",
    "print (\"File loaded with \",dfall.shape[0], \" events \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it should tell you \"File Loaded with XXX events\". If not, it could not access the datafile, no point going further !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump list of feature\n",
    "dfall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "j9l7wkkorlHe",
    "outputId": "c386c1e1-2ff2-4878-cd92-d3243cd87d2e"
   },
   "outputs": [],
   "source": [
    "#examine first few events\n",
    "dfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "Oz-lWJhgrlHg",
    "outputId": "0d44f143-0518-4d35-8c25-5b5f5021c567"
   },
   "outputs": [],
   "source": [
    "#examine feature distribution\n",
    "dfall.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "lQsalTmorlHj",
    "outputId": "7d445307-6198-467a-9e59-03621e15731b"
   },
   "outputs": [],
   "source": [
    "label_weights = (dfall[dfall.label==0].mcWeight.sum(), dfall[dfall.label==1].mcWeight.sum() ) \n",
    "print(\"total label weights\",label_weights)\n",
    "\n",
    "\n",
    "label_nevents = (dfall[dfall.label==0].shape[0], dfall[dfall.label==1].shape[0] )\n",
    "print (\"total class number of events\",label_nevents)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtI5u5GErlHq"
   },
   "source": [
    "## Event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kaO2JM1hrlHr",
    "outputId": "65aa5d10-0fed-4ec0-abad-77ff6c7b7dc2"
   },
   "outputs": [],
   "source": [
    "print (dfall.shape)\n",
    "fulldata=dfall[dfall.lep_n==2] # only keep events with exactly two leptons \n",
    "print (fulldata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iMQpKhDKrlH0",
    "outputId": "db60a15f-808a-446d-cabe-0a89453f3d82"
   },
   "outputs": [],
   "source": [
    "#hide label and weights in separate vectors\n",
    "#they are not real features\n",
    "\n",
    "#WARNING : there should be no selection nor shuffling later on !\n",
    "target = fulldata[\"label\"]\n",
    "del fulldata[\"label\"]\n",
    "\n",
    "#hide weight in separate vector\n",
    "weights = fulldata[\"mcWeight\"]\n",
    "del fulldata[\"mcWeight\"]\n",
    "fulldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nviyIMgerlH3"
   },
   "source": [
    "\n",
    "# DO NOT MODIFY ANYTHING ABOVE\n",
    "... and always rerun from this cell whenever you change something below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6e0Hlpv6rlH4",
    "outputId": "1d9fa3fc-da90-48a9-c2b9-facfaa2136ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for simplicity of the exercise only keep some features\n",
    "# this is actually making a copy from fulldata\n",
    "data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1'])\n",
    "#data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
    "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']\n",
    "print (data.shape)\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "To be switched on in a second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
    "    #data[\"lep_deltaphi\"]=data.lep_phi_1-data.lep_phi_0\n",
    "\n",
    "\n",
    "    print (data.shape)\n",
    "    display(data.head())\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "    ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),color='b',alpha=0.5,density=True)\n",
    "    data[target==1].hist(weights=weights[target==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o2mf1bLVrlH7",
    "outputId": "869412a8-76fd-4946-87c2-64234e418c96"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#_, ax = plt.subplots()\n",
    "\n",
    "ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),color='b',alpha=0.5,density=True)\n",
    "ax=ax.flatten()[:data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "data[target==1].hist(weights=weights[target==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZM-E5H4rlH-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "## Transformation of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Y9j5hdrmrlID",
    "outputId": "b8b5ae1c-d0bf-4a9b-915c-98531b8ae56b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_size = 0.75 # fraction of sample used for training\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
    "    train_test_split(data, target, weights, train_size=train_size)\n",
    "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "y_train, y_test, weights_train, weights_test = \\\n",
    "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (weights_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "print (weights_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scale to mean 0 and variance 1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # applied the transformation calculated the line above\n",
    "\n",
    "\n",
    "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
    "\n",
    "print (\"class_weights_train:\",class_weights_train)\n",
    "for i in range(len(class_weights_train)):\n",
    "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
    "    weights_test[y_test == i] *= 1/(1-train_size) # increase test weight to compensate for sampling\n",
    "    \n",
    "print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
    "print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())\n",
    "print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
    "print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Testing BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load significance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "def amsasimov(s,b): # asimov (or Poisson) significance\n",
    "        if b<=0 or s<=0:\n",
    "            return 0\n",
    "        try:\n",
    "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
    "        except ValueError:\n",
    "            print(1+float(s)/b)\n",
    "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
    "        #return s/sqrt(s+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4DfF0ISrlIN"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "nqMCgvbkrlIN",
    "outputId": "e21e24af-3026-42ec-bb55-8d52bbdbdce9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(31415) # set the random seed\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "xgb = XGBClassifier(tree_method=\"hist\")\n",
    "#xgb = XGBClassifier(tree_method=\"hist\",max_depth=12) # HPO, check on the web for other parameters\n",
    "# not a bad idea to check for bugs without hist\n",
    "\n",
    "\n",
    "starting_time = time.time( )\n",
    "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values)\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)\n",
    "\n",
    "y_pred_xgb = xgb.predict_proba(X_test)[:,1]\n",
    "y_pred_xgb = y_pred_xgb.ravel()\n",
    "y_pred_train_xgb = xgb.predict_proba(X_train)[:,1].ravel()\n",
    "auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb)\n",
    "print(\"auc test:\",auc_test_xgb)\n",
    "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,))\n",
    "\n",
    "int_pred_test_sig_xgb = [weights_test[(y_test ==1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg_xgb = [weights_test[(y_test ==0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "vamsasimov_xgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb,int_pred_test_bkg_xgb)]\n",
    "significance_xgb = max(vamsasimov_xgb)\n",
    "Z = significance_xgb\n",
    "print(\"Z:\",Z)\n",
    "# To save model\n",
    "xgb.save_model(\"XGBoost.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXTRSemqrlIR"
   },
   "outputs": [],
   "source": [
    "#gridSearchCV for advanced HPO, check on the web for other parameters\n",
    "\n",
    "if True:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #param_list = {'max_depth': [3, 6, 7], 'subsample': [0.7, 1],\n",
    "    #                    'learning_rate': [0.05, 0.3], 'n_estimators': [10, 50, 200]}\n",
    "    # all possible combinations of parameters will be used, can take a few minutes\n",
    "    #param_list_XGB = {'max_depth': [6,10], 'subsample': [0.7,1],\n",
    "    #                    'learning_rate': [0.05, 0.3], 'max_leaves': [50, 200]}\n",
    "    param_list_XGB = {'max_depth': [2,5],\"n_estimators\" : [50,100]}\n",
    "\n",
    "\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier(tree_method=\"hist\"), \n",
    "                        param_grid = param_list_XGB, \n",
    "                        scoring='roc_auc',n_jobs=-1,iid=False, cv=2)\n",
    "    gsearch1.fit(X_train,y_train, weights_train)\n",
    "    print (gsearch1.best_params_)\n",
    "    print (gsearch1.best_score_)\n",
    "\n",
    "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
    "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmadcFRJrlIT"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "9YmfuiwUrlIU",
    "outputId": "ee29b618-4291-4879-c1ce-b45adbfedc8e"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "#gbm = lgb.LGBMClassifier()\n",
    "gbm = lgb.LGBMClassifier()\n",
    "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web for other parameters\n",
    "\n",
    "\n",
    "starting_time = time.time( )\n",
    "\n",
    "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
    "#gbm.fit(X_train, y_train.values) #ma\n",
    "\n",
    "\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)\n",
    "\n",
    "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
    "y_pred_gbm = y_pred_gbm.ravel()\n",
    "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
    "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm)\n",
    "print(\"auc test:\",auc_test_gbm)\n",
    "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
    "\n",
    "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
    "significance_gbm = max(vamsasimov_gbm)\n",
    "Z = significance_gbm\n",
    "print(\"Z:\",Z)\n",
    "# To save model\n",
    "gbm.booster_.save_model(\"LightGBM.model\")\n",
    "\n",
    "\n",
    "# Why is lightgbm worse ? Is it because of negative weights ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEm5cudSrlIW"
   },
   "outputs": [],
   "source": [
    "#gridSearchCV for advanced HPO\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    #param_list_GBM = {'max_depth': [3, 6, 7],\n",
    "    #                     'learning_rate': [0.01, 0.05, 0.1, 0.3, 1], 'n_estimators': [10, 20, 40, 50, 200]}\n",
    "    param_list_GBM = {'max_depth': [6,10], \n",
    "                        'learning_rate': [0.05, 0.3], 'n_estimators': [10, 200]}\n",
    "\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier(), \n",
    "    param_grid = param_list_GBM, scoring='roc_auc',n_jobs=4,iid=False, cv=2)\n",
    "    gsearch1.fit(X_train,y_train, weights_train)\n",
    "    print (gsearch1.best_params_)\n",
    "    print (gsearch1.best_score_)\n",
    "\n",
    "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
    "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "zmJxYTUmrlIZ",
    "outputId": "145ab22e-682b-4e66-aca6-c542402f52c8"
   },
   "outputs": [],
   "source": [
    "print('Best significance found are:')\n",
    "print('LightGBM: ', significance_gbm)\n",
    "print('XGBoost : ', significance_xgb)\n",
    "print('Best auc train found are:')\n",
    "print('LightGBM: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
    "print('XGBoost: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-LB9cbErlIb"
   },
   "source": [
    "## Some nice plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load score plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities\n",
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "# Plot score for signal and background, comparing training and testing\n",
    "def compare_train_test(y_pred_train, y_train, y_pred, y_test, high_low=(0,1), bins=30, xlabel=\"\", ylabel=\"Arbitrary units\", title=\"\", weights_train=np.array([]), weights_test=np.array([])):\n",
    "    if weights_train.size != 0:\n",
    "        weights_train_signal = weights_train[y_train == 1]\n",
    "        weights_train_background = weights_train[y_train == 0]\n",
    "    else:\n",
    "        weights_train_signal = None\n",
    "        weights_train_background = None\n",
    "    plt.hist(y_pred_train[y_train == 1],\n",
    "                 color='r', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', density=True,\n",
    "                 label='S (train)', weights=weights_train_signal) # alpha is transparancy\n",
    "    plt.hist(y_pred_train[y_train == 0],\n",
    "                 color='b', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', density=True,\n",
    "                 label='B (train)', weights=weights_train_background)\n",
    "\n",
    "    if weights_test.size != 0:\n",
    "        weights_test_signal = weights_test[y_test == 1]\n",
    "        weights_test_background = weights_test[y_test == 0]\n",
    "    else:\n",
    "        weights_test_signal = None\n",
    "        weights_test_background = None\n",
    "    hist, bins = np.histogram(y_pred[y_test == 1],\n",
    "                                  bins=bins, range=high_low, density=True, weights=weights_test_signal)\n",
    "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    #width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "\n",
    "    hist, bins = np.histogram(y_pred[y_test == 0],\n",
    "                                  bins=bins, range=high_low, density=True, weights=weights_test_background)\n",
    "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    #width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "colab_type": "code",
    "id": "kEQ342s-rlIe",
    "outputId": "16ba71ab-a415-4aeb-af3b-3e6674009181"
   },
   "outputs": [],
   "source": [
    "\n",
    "compare_train_test(y_pred_train_xgb, y_train, y_pred_xgb, y_test, xlabel=\"XGboost score\", title=\"XGboost\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
    "plt.savefig(\"Score_BDT_XGBoost_Hist.pdf\")\n",
    "plt.show()\n",
    "compare_train_test(y_pred_train_gbm, y_train, y_pred_gbm, y_test, xlabel=\"LightGBM score\", title=\"LightGBM\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
    "plt.savefig(\"Score_BDT_LightGBM.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "tUKIWu2orlIg",
    "outputId": "490e7dcd-b1c8-4f40-ab31-57ee3726208a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "lw = 2\n",
    "\n",
    "fpr_gbm,tpr_gbm,_ = roc_curve(y_true=y_test, y_score=y_pred_gbm,)#,sample_weight=weights_test.values)\n",
    "fpr_xgb,tpr_xgb,_ = roc_curve(y_true=y_test, y_score=y_pred_xgb,)#,sample_weight=weights_test.values)\n",
    "plt.plot(fpr_gbm, tpr_gbm, color='darkorange',lw=lw, label='LightGBM (AUC  = {})'.format(np.round(auc_test_gbm,decimals=2)))\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='darkgreen',lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb,decimals=2)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#import os\n",
    "#new_dir = \"Plots/Comparing\" \n",
    "#if not os.path.isdir(new_dir):\n",
    "#    os.mkdir(new_dir)\n",
    "plt.savefig(\"ROC_comparing.pdf\")\n",
    "plt.show() # blue line = random classification -> maximize true positive rate while miniize false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "CGF3k0KJrlIi",
    "outputId": "1fa7c311-85e4-4d9e-fc56-0c38c2ec27da"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_gbm, label='LightGBM (Z = {})'.format(np.round(significance_gbm,decimals=2)))\n",
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_xgb, label='XGBoost (Z = {})'.format(np.round(significance_xgb,decimals=2)))\n",
    "\n",
    "plt.title(\"BDT Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Significance_comparing.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "id": "QN2h9y5JrlIm",
    "outputId": "e3b688b2-a639-4d07-83c5-92ef93432811"
   },
   "outputs": [],
   "source": [
    "plt.bar(data.columns.values, xgb.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances XGBoost Hist\")\n",
    "#plt.savefig(new_dir + \"/VarImp_BDT_XGBoost_Hist.pdf\",bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.bar(data.columns.values, gbm.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances LightGBM\")\n",
    "#plt.savefig(new_dir + \"/VarImp_BDT_LightGBM.pdf\",bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7WxH3bDrlIp"
   },
   "source": [
    "# Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "sMzhd_vWrlIq",
    "outputId": "9a1a1b9b-5665-4682-e12c-3bf39c1f8e8f"
   },
   "outputs": [],
   "source": [
    "#TODO replace by Aishik's stuff\n",
    "#a bit slow\n",
    "#if COLAB:\n",
    "if False:  \n",
    "    import eli5\n",
    "    from eli5.sklearn import PermutationImportance\n",
    "    file = open('html_original.html', 'w')\n",
    "    file.write(HTML('<h1>Permutation importances XGBoost</h1>').data)\n",
    "    perm_xgb = PermutationImportance(xgb, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
    "    html_xgb = eli5.show_weights(perm_xgb, feature_names = data.columns.values).data\n",
    "    #with open('html_xgb.html', 'w') as f:\n",
    "    #    f.write(HTML('<h1>Permutation importances XGBoost Hist</h1>').data)\n",
    "    #    f.write(html_xgb)\n",
    "    file.write(html_xgb)\n",
    "    perm_gbm = PermutationImportance(gbm, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
    "    html_gbm = eli5.show_weights(perm_gbm, feature_names = data.columns.values).data\n",
    "    #with open('html_gbm.html', 'w') as f:\n",
    "    #    f.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
    "    #    f.write(html_gbm)\n",
    "    file.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
    "    file.write(html_gbm)\n",
    "    print (\"Permutation importances XGBoost\")\n",
    "    display(eli5.show_weights(perm_xgb, feature_names = data.columns.values))\n",
    "    print (\"Permutation importances LightGBM\")\n",
    "    display(eli5.show_weights(perm_gbm, feature_names = data.columns.values))\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_BDT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
